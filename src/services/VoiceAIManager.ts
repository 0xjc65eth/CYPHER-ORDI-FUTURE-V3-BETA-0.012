import { ElevenLabsRealService } from './ElevenLabsRealService';

export interface VoiceAIConfig {
  recognition: 'continuous' | 'single';
  synthesis: 'elevenlabs' | 'browser';
  language: string;
  apiKey?: string;
}

export class VoiceAIManager {
  private recognition: any;
  private synthesis: ElevenLabsRealService;
  private language: string;
  private isListening: boolean = false;
  
  constructor(config: VoiceAIConfig) {
    this.language = config.language || 'pt-BR';
    
    // Configurar reconhecimento de voz (Web Speech API)
    if (typeof window !== 'undefined' && 'webkitSpeechRecognition' in window) {
      const SpeechRecognition = (window as any).webkitSpeechRecognition;
      this.recognition = new SpeechRecognition();
      this.recognition.continuous = config.recognition === 'continuous';
      this.recognition.interimResults = true;
      this.recognition.lang = this.language;
    }
    
    // Configurar s칤ntese de voz
    this.synthesis = new ElevenLabsRealService({ 
      apiKey: config.apiKey || process.env.NEXT_PUBLIC_ELEVENLABS_API_KEY || ''
    });
  }
  
  async startListening(): Promise<void> {
    if (!this.recognition) {
      throw new Error('Reconhecimento de voz n칚o suportado neste navegador');
    }
    
    return new Promise((resolve, reject) => {
      this.recognition.start();
      this.isListening = true;
      
      this.recognition.onstart = () => {
        console.log('游꿗 Ouvindo...');
        resolve();
      };
      
      this.recognition.onerror = (event: any) => {
        console.error('Erro no reconhecimento:', event.error);
        this.isListening = false;
        reject(event.error);
      };
    });
  }
  
  stopListening(): void {
    if (this.recognition && this.isListening) {
      this.recognition.stop();
      this.isListening = false;
    }
  }
  
  onResult(callback: (text: string) => void): void {
    if (!this.recognition) return;
    
    this.recognition.onresult = (event: any) => {
      const last = event.results.length - 1;
      const text = event.results[last][0].transcript;
      callback(text);
    };
  }
  
  async processVoiceInput(audioBlob: Blob): Promise<string> {
    // Para 치udio gravado, usar a API do ElevenLabs
    try {
      const text = await this.synthesis.transcribe(audioBlob);
      return text;
    } catch (error) {
      console.error('Erro ao transcrever 치udio:', error);
      throw error;
    }
  }
  
  async generateResponse(text: string): Promise<{
    response: string;
    emotion?: string;
  }> {
    // Analisar a inten칞칚o e gerar resposta com g칤rias
    const lowerText = text.toLowerCase();
    let response = '';
    let emotion = 'neutral';
    
    if (lowerText.includes('ol치') || lowerText.includes('oi')) {
      response = "E a칤, mano! T칪 ligado que tu quer ganhar uma grana. Bora nessa! 游";
      emotion = 'friendly';
    } else if (lowerText.includes('bot') || lowerText.includes('trading')) {
      response = "Quer que eu ative o bot de trading pra voc칡? 칄 s칩 falar 'iniciar bot' que eu coloco ele pra trabalhar!";
      emotion = 'excited';
    } else if (lowerText.includes('mercado') || lowerText.includes('bitcoin')) {
      response = "O mercado t치 bombando, mano! Bitcoin subindo, altcoins seguindo... Quer uma an치lise completa?";
      emotion = 'analytical';
    } else {
      response = "T칪 aqui pra te ajudar, par칞a! Pode perguntar sobre trading, mercado, ou qualquer parada de crypto!";
      emotion = 'helpful';
    }
    
    return { response, emotion };
  }
  
  async speak(text: string, emotion?: string): Promise<void> {
    try {
      const audio = await this.synthesis.synthesize(text, emotion);
      
      if (typeof window !== 'undefined') {
        const audioUrl = URL.createObjectURL(audio);
        const audioElement = new Audio(audioUrl);
        
        return new Promise((resolve, reject) => {
          audioElement.onended = () => {
            URL.revokeObjectURL(audioUrl);
            resolve();
          };
          
          audioElement.onerror = (error) => {
            URL.revokeObjectURL(audioUrl);
            reject(error);
          };
          
          audioElement.play();
        });
      }
    } catch (error) {
      console.error('Erro ao falar:', error);
      // Fallback para s칤ntese do navegador
      if (typeof window !== 'undefined' && 'speechSynthesis' in window) {
        const utterance = new SpeechSynthesisUtterance(text);
        utterance.lang = this.language;
        utterance.rate = 1.1;
        utterance.pitch = 1.0;
        window.speechSynthesis.speak(utterance);
      }
    }
  }
  
  setLanguage(language: string): void {
    this.language = language;
    if (this.recognition) {
      this.recognition.lang = language;
    }
  }
  
  isSupported(): boolean {
    return typeof window !== 'undefined' && 
           ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window);
  }
}

export default VoiceAIManager;